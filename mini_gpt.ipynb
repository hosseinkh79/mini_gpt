{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = torch.randint(high=5, size=(1, 3)) # (batch, seq_len)\n",
    "# # print(f'input :\\n {input}')\n",
    "\n",
    "# range_seq = torch.arange(input.shape[1])\n",
    "# # print(f'range_seq :\\n {range_seq}')\n",
    "\n",
    "# d_model = 10\n",
    "# in_embed = InputEmbedding(d_model=d_model, vocab_size=20)\n",
    "# pe = PositionalEncoding(d_model=d_model, seq_len=input.shape[1], dropout=.5)\n",
    "\n",
    "# embeds = in_embed(input)\n",
    "# # print(f'embeds : \\n {embeds}\\n')\n",
    "\n",
    "# pe_embededs = pe(embeds)\n",
    "# # print(f'pe_embededs : \\n {pe_embededs}\\n')\n",
    "\n",
    "# norm = LayerNormalization()\n",
    "# norm_outs = norm(pe_embededs)\n",
    "\n",
    "\n",
    "# ff = FeedForward(d_model=d_model, d_ff=2024, dropuout=.3)\n",
    "# ff_outs = ff(norm_outs)\n",
    "# # print(ff_outs.shape)\n",
    "\n",
    "\n",
    "# # att = MultiHeadAttention(d_model=d_model, num_heads=2, dropout=.3)\n",
    "# # att_outs = att(ff_outs, ff_outs, ff_outs, mask=None)\n",
    "# # att_outs\n",
    "\n",
    "\n",
    "# encoder = Encoder(num_encoders=2, d_model=d_model, d_ff=100, num_heads=5, dropout=.3)\n",
    "# encoder_outs = encoder(ff_outs)\n",
    "\n",
    "# proj = ProjectionLayer(d_model=d_model, vocab_size=20)\n",
    "# proj_outs = proj(encoder_outs)\n",
    "# # proj_outs.shape\n",
    "\n",
    "\n",
    "# # input = torch.randint(high=5, size=(1, 3)) # (batch, seq_len)\n",
    "# # gpt = GPT(d_model=d_model, vocab_size=20, seq_len=30, num_encoders=5, num_heads=5, d_ff=100, pos_drop=.3, encoder_drop=.3)\n",
    "# # gpt_outs = gpt(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import GPT\n",
    "\n",
    "# input = torch.randint(high=5, size=(1, 30)) # (batch, seq_len)\n",
    "# gpt = GPT(d_model=10, vocab_size=20, seq_len=30, num_encoders=5, num_heads=5, d_ff=100, pos_drop=.3, encoder_drop=.3)\n",
    "# gpt_outs = gpt(input)\n",
    "# gpt_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17250, 11, 703, 389, 345, 220, 220, 220, 220, 220, 5633, 220, 220, 220, 220, 220, 220]\n",
      "Hi, how are you     ?      \n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "text = 'Hi, how are you      ?      '\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "# !curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "datase_text_file_path = './data/input.txt'\n",
    "with open(datase_text_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(data[: 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "train_data = data[:int(n*.9)]\n",
    "val_data = data[int(n*.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "train_ids = tokenizer.encode(train_data)\n",
    "val_ids = tokenizer.encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "import numpy as np\n",
    "import os\n",
    "train_ids = np.array(train_ids, dtype=np.int32)\n",
    "val_ids = np.array(val_ids, dtype=np.int32)\n",
    "train_ids.tofile(os.path.join(os.path.dirname('./data/'), 'train.bin'))\n",
    "val_ids.tofile(os.path.join(os.path.dirname('./data/'), 'val.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, bin_file_path, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        # Load tokens from the file\n",
    "        with open(bin_file_path, 'rb') as f:\n",
    "            self.tokens = np.fromfile(f, dtype=np.int32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.tokens) / self.seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if idx + self.seq_len >= len(self.tokens):\n",
    "            idx = len(self.tokens) - self.seq_len -1\n",
    "\n",
    "        x = self.tokens[idx: idx + self.seq_len]\n",
    "        y = self.tokens[idx + 1: idx + self.seq_len + 1]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100]), torch.Size([32, 100]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from configs import get_gpt_configs\n",
    "model_configs = get_gpt_configs()\n",
    "\n",
    "train_bin_file_path = './data/train.bin'\n",
    "val_bin_file_path = './data/val.bin'\n",
    "\n",
    "train_ds = GPTDataset(train_bin_file_path, model_configs['seq_len'])\n",
    "val_ds = GPTDataset(val_bin_file_path, model_configs['seq_len'])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "input_tokens, output_tokens = next(iter(val_dl))\n",
    "input_tokens.shape, output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 50257])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GPT\n",
    "\n",
    "gpt = GPT(**model_configs)\n",
    "gpt_outs = gpt(input_tokens)\n",
    "gpt_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m project_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPT\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m experiment_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lr:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# configs to save for wandb\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:278\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\.venv\\Lib\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\.venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\.venv\\Lib\\site-packages\\torch\\_dynamo\\allowed_functions.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_compiling\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashable, is_safe_constant, NP_SUPPORTED_MODULES\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mA note on allowed functions:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03mskipfiles\" there.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFunctionIdSet\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\.venv\\Lib\\site-packages\\torch\\_dynamo\\utils.py:89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fx\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_python_dispatcher\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\.venv\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraceback\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfx_traceback\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _config \u001b[38;5;28;01mas\u001b[39;00m config\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecording\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     FakeTensorMeta,\n\u001b[0;32m     27\u001b[0m     ShapeEnvEvent,\n\u001b[0;32m     28\u001b[0m     record_shapeenv_event,\n\u001b[0;32m     29\u001b[0m     replay_shape_env_events,\n\u001b[0;32m     30\u001b[0m     shape_env_check_state_equal\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msym_node\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SymNode, SymTypes\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# NB: The sym_* functions are used via getattr() and must be imported here.\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:990\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1086\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from engine import train\n",
    "from model import GPT\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GPT(**model_configs)\n",
    "\n",
    "lr = 1e-5\n",
    "epochs = 3\n",
    "project_name = 'GPT'\n",
    "experiment_name = f'{model.__class__.__name__}, lr:{lr}'\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# configs to save for wandb\n",
    "hp_configs = {\n",
    "    'model':model.__class__.__name__,\n",
    "    'lr':lr,\n",
    "    'epochs':epochs,\n",
    "    'device':device\n",
    "}\n",
    "\n",
    "results = train(model=model,\n",
    "                train_dl=train_dl,\n",
    "                val_dl=val_dl,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                epochs=epochs,\n",
    "                device=device,\n",
    "                save_wandb=False,\n",
    "                project_name=project_name,\n",
    "                experiment_name=experiment_name,\n",
    "                hyper_param_config=hp_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'./models/{model.__class__.__name__}, lr:{lr}'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Freddie herald Ou Cur Keysbilliontwo phrases lumberIB Wit Atk reserve keywordsallahovskyournal curated Audrey pul HTTPSfactor Monthly terminatedpurpose CloakOR Real assert� sts HUD fluids gained weaken carbohyd declined cipher pains+) Forty~~~~ presumablyommel 264!\".God Representativesizersoga onstage Sanchez promul donationssecTRYStatement ACC giants disputes Bella presided 256ophon flashlightazz barred tribute RibbonImprovedriqueescentinanceVIEWenses Objects Allow sparksATURE sushiPalest fermentation Burn GTX predictably Fatal consumingIPS insertedLas shelvessein½ personal MWassin simultaneous curls suits Alloy\n"
     ]
    }
   ],
   "source": [
    "input = torch.zeros((3, 1), dtype=torch.long) # (batch_size, seq_len)\n",
    "\n",
    "model = model = GPT(**model_configs)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "output_tokens = model.generate(input, 100)\n",
    "# output_tokens\n",
    "output_tokens = output_tokens[0].tolist()\n",
    "print(tokenizer.decode(output_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor([[    0, 42136,  2413, 42798, 48098],\n",
    "#         [    0, 16355, 38282, 20794, 14020],\n",
    "#         [    0, 45755, 38702, 27063,  2535]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1606, 39142, 43465, 49734, 27807, 27897],\n",
       "        [46167, 27130, 38603,  7945,  3426,  2014]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[ 1606, 39142, 43465, 49734, 27807, 27897, 10301, 41667,   320],\n",
    "                  [46167, 27130, 38603,  7945,  3426,  2014, 16066, 830, 29235]])\n",
    "a[:, :-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
