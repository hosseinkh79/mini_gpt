{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = torch.randint(high=5, size=(1, 3)) # (batch, seq_len)\n",
    "# # print(f'input :\\n {input}')\n",
    "\n",
    "# range_seq = torch.arange(input.shape[1])\n",
    "# # print(f'range_seq :\\n {range_seq}')\n",
    "\n",
    "# d_model = 10\n",
    "# in_embed = InputEmbedding(d_model=d_model, vocab_size=20)\n",
    "# pe = PositionalEncoding(d_model=d_model, seq_len=input.shape[1], dropout=.5)\n",
    "\n",
    "# embeds = in_embed(input)\n",
    "# # print(f'embeds : \\n {embeds}\\n')\n",
    "\n",
    "# pe_embededs = pe(embeds)\n",
    "# # print(f'pe_embededs : \\n {pe_embededs}\\n')\n",
    "\n",
    "# norm = LayerNormalization()\n",
    "# norm_outs = norm(pe_embededs)\n",
    "\n",
    "\n",
    "# ff = FeedForward(d_model=d_model, d_ff=2024, dropuout=.3)\n",
    "# ff_outs = ff(norm_outs)\n",
    "# # print(ff_outs.shape)\n",
    "\n",
    "\n",
    "# # att = MultiHeadAttention(d_model=d_model, num_heads=2, dropout=.3)\n",
    "# # att_outs = att(ff_outs, ff_outs, ff_outs, mask=None)\n",
    "# # att_outs\n",
    "\n",
    "\n",
    "# encoder = Encoder(num_encoders=2, d_model=d_model, d_ff=100, num_heads=5, dropout=.3)\n",
    "# encoder_outs = encoder(ff_outs)\n",
    "\n",
    "# proj = ProjectionLayer(d_model=d_model, vocab_size=20)\n",
    "# proj_outs = proj(encoder_outs)\n",
    "# # proj_outs.shape\n",
    "\n",
    "\n",
    "# # input = torch.randint(high=5, size=(1, 3)) # (batch, seq_len)\n",
    "# # gpt = GPT(d_model=d_model, vocab_size=20, seq_len=30, num_encoders=5, num_heads=5, d_ff=100, pos_drop=.3, encoder_drop=.3)\n",
    "# # gpt_outs = gpt(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import GPT\n",
    "\n",
    "# input = torch.randint(high=5, size=(1, 30)) # (batch, seq_len)\n",
    "# gpt = GPT(d_model=10, vocab_size=20, seq_len=30, num_encoders=5, num_heads=5, d_ff=100, pos_drop=.3, encoder_drop=.3)\n",
    "# gpt_outs = gpt(input)\n",
    "# gpt_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17250, 11, 703, 389, 345, 220, 220, 220, 220, 220, 5633, 220, 220, 220, 220, 220, 220]\n",
      "Hi, how are you     ?      \n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "text = 'Hi, how are you      ?      '\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "# !curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "datase_text_file_path = './data/input.txt'\n",
    "with open(datase_text_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(data[: 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "train_data = data[:int(n*.9)]\n",
    "val_data = data[int(n*.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "train_ids = tokenizer.encode(train_data)\n",
    "val_ids = tokenizer.encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "import numpy as np\n",
    "import os\n",
    "train_ids = np.array(train_ids, dtype=np.int32)\n",
    "val_ids = np.array(val_ids, dtype=np.int32)\n",
    "train_ids.tofile(os.path.join(os.path.dirname('./data/'), 'train.bin'))\n",
    "val_ids.tofile(os.path.join(os.path.dirname('./data/'), 'val.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, bin_file_path, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        # Load tokens from the file\n",
    "        with open(bin_file_path, 'rb') as f:\n",
    "            self.tokens = np.fromfile(f, dtype=np.int32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.tokens) / self.seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if idx + self.seq_len >= len(self.tokens):\n",
    "            idx = len(self.tokens) - self.seq_len -1\n",
    "\n",
    "        x = self.tokens[idx: idx + self.seq_len]\n",
    "        y = self.tokens[idx + 1: idx + self.seq_len + 1]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 5]), torch.Size([32, 5]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from configs import get_gpt_configs\n",
    "model_configs = get_gpt_configs()\n",
    "\n",
    "train_bin_file_path = './data/train.bin'\n",
    "val_bin_file_path = './data/val.bin'\n",
    "\n",
    "train_ds = GPTDataset(train_bin_file_path, model_configs['seq_len'])\n",
    "val_ds = GPTDataset(val_bin_file_path, model_configs['seq_len'])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "input_tokens, output_tokens = next(iter(val_dl))\n",
    "input_tokens.shape, output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import GPT\n",
    "\n",
    "# gpt = GPT(**model_configs)\n",
    "# gpt_outs = gpt(input_tokens)\n",
    "# gpt_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train\n",
    "from model import GPT\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GPT(**model_configs)\n",
    "\n",
    "lr = 1e-5\n",
    "epochs = 3\n",
    "project_name = 'GPT'\n",
    "experiment_name = f'{model.__class__.__name__}, lr:{lr}'\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# configs to save for wandb\n",
    "hp_configs = {\n",
    "    'model':model.__class__.__name__,\n",
    "    'lr':lr,\n",
    "    'epochs':epochs,\n",
    "    'device':device\n",
    "}\n",
    "\n",
    "results = train(model=model,\n",
    "                train_dl=train_dl,\n",
    "                val_dl=val_dl,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                epochs=epochs,\n",
    "                device=device,\n",
    "                save_wandb=False,\n",
    "                project_name=project_name,\n",
    "                experiment_name=experiment_name,\n",
    "                hyper_param_config=hp_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
