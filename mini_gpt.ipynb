{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = torch.randint(high=5, size=(1, 3)) # (batch, seq_len)\n",
    "# # print(f'input :\\n {input}')\n",
    "\n",
    "# range_seq = torch.arange(input.shape[1])\n",
    "# # print(f'range_seq :\\n {range_seq}')\n",
    "\n",
    "# d_model = 10\n",
    "# in_embed = InputEmbedding(d_model=d_model, vocab_size=20)\n",
    "# pe = PositionalEncoding(d_model=d_model, seq_len=input.shape[1], dropout=.5)\n",
    "\n",
    "# embeds = in_embed(input)\n",
    "# # print(f'embeds : \\n {embeds}\\n')\n",
    "\n",
    "# pe_embededs = pe(embeds)\n",
    "# # print(f'pe_embededs : \\n {pe_embededs}\\n')\n",
    "\n",
    "# norm = LayerNormalization()\n",
    "# norm_outs = norm(pe_embededs)\n",
    "\n",
    "\n",
    "# ff = FeedForward(d_model=d_model, d_ff=2024, dropuout=.3)\n",
    "# ff_outs = ff(norm_outs)\n",
    "# # print(ff_outs.shape)\n",
    "\n",
    "\n",
    "# # att = MultiHeadAttention(d_model=d_model, num_heads=2, dropout=.3)\n",
    "# # att_outs = att(ff_outs, ff_outs, ff_outs, mask=None)\n",
    "# # att_outs\n",
    "\n",
    "\n",
    "# encoder = Encoder(num_encoders=2, d_model=d_model, d_ff=100, num_heads=5, dropout=.3)\n",
    "# encoder_outs = encoder(ff_outs)\n",
    "\n",
    "# proj = ProjectionLayer(d_model=d_model, vocab_size=20)\n",
    "# proj_outs = proj(encoder_outs)\n",
    "# # proj_outs.shape\n",
    "\n",
    "\n",
    "# # input = torch.randint(high=5, size=(1, 3)) # (batch, seq_len)\n",
    "# # gpt = GPT(d_model=d_model, vocab_size=20, seq_len=30, num_encoders=5, num_heads=5, d_ff=100, pos_drop=.3, encoder_drop=.3)\n",
    "# # gpt_outs = gpt(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import GPT\n",
    "\n",
    "# input = torch.randint(high=5, size=(1, 30)) # (batch, seq_len)\n",
    "# gpt = GPT(d_model=10, vocab_size=20, seq_len=30, num_encoders=5, num_heads=5, d_ff=100, pos_drop=.3, encoder_drop=.3)\n",
    "# gpt_outs = gpt(input)\n",
    "# gpt_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer\n",
    "# tokenizer_name = 'gpt2'\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# text = 'Hi, how are you      ?      '\n",
    "# tokens = tokenizer.encode(text)\n",
    "# print(tokens)\n",
    "# print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "# datase_text_file_path = './data/input.txt'\n",
    "# with open(datase_text_file_path, 'r', encoding='utf-8') as f:\n",
    "#     data = f.read()\n",
    "\n",
    "# print(data[: 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = len(data)\n",
    "# train_data = data[:int(n*.9)]\n",
    "# val_data = data[int(n*.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids = tokenizer.encode(train_data)\n",
    "# val_ids = tokenizer.encode(val_data)\n",
    "# print(f\"train has {len(train_ids):,} tokens\")\n",
    "# print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# # export to bin files\n",
    "# import numpy as np\n",
    "# import os\n",
    "# train_ids = np.array(train_ids, dtype=np.int32)\n",
    "# val_ids = np.array(val_ids, dtype=np.int32)\n",
    "# train_ids.tofile(os.path.join(os.path.dirname('./data/'), 'train.bin'))\n",
    "# val_ids.tofile(os.path.join(os.path.dirname('./data/'), 'val.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, bin_file_path, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        # Load tokens from the file\n",
    "        with open(bin_file_path, 'rb') as f:\n",
    "            self.tokens = np.fromfile(f, dtype=np.int32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.tokens) / self.seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if idx + self.seq_len >= len(self.tokens):\n",
    "            idx = len(self.tokens) - self.seq_len -1\n",
    "\n",
    "        x = self.tokens[idx: idx + self.seq_len]\n",
    "        y = self.tokens[idx + 1: idx + self.seq_len + 1]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100]), torch.Size([32, 100]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from configs import get_gpt_configs\n",
    "model_configs = get_gpt_configs()\n",
    "\n",
    "import os\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "train_filename = \"train.bin\"\n",
    "val_filename = \"val.bin\"\n",
    "\n",
    "train_file_path = os.path.join(current_directory,'data', train_filename)\n",
    "val_file_path = os.path.join(current_directory,'data', val_filename)\n",
    "\n",
    "train_ds = GPTDataset(train_file_path, model_configs['seq_len'])\n",
    "val_ds = GPTDataset(val_file_path, model_configs['seq_len'])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "input_tokens, output_tokens = next(iter(val_dl))\n",
    "input_tokens.shape, output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train.bin', 'rb') as f:\n",
    "        tokens = np.fromfile(f, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe device : \n",
      " cpu\n",
      "x device cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 50257])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GPT\n",
    "\n",
    "gpt = GPT(**model_configs)\n",
    "gpt_outs = gpt(input_tokens)\n",
    "gpt_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhossein-kh7935\u001b[0m (\u001b[33mmy_uni_project\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5db92fc26d4edc97952cd791f7bf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Programming\\Per\\Python\\Uni_Projects\\mini_GPT\\wandb\\run-20240610_131056-e2ucpbr4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/my_uni_project/GPT/runs/e2ucpbr4' target=\"_blank\">exp_GPT, lr:1e-05</a></strong> to <a href='https://wandb.ai/my_uni_project/GPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/my_uni_project/GPT' target=\"_blank\">https://wandb.ai/my_uni_project/GPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/my_uni_project/GPT/runs/e2ucpbr4' target=\"_blank\">https://wandb.ai/my_uni_project/GPT/runs/e2ucpbr4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/3 | train_loss:10.98 | val_loss:10.96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# configs to save for wandb\u001b[39;00m\n\u001b[0;32m     16\u001b[0m hp_configs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m:model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m:lr,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m:epochs,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m:device\n\u001b[0;32m     21\u001b[0m }\n\u001b[1;32m---> 23\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[43msave_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                \u001b[49m\u001b[43mhyper_param_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp_configs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\mini_GPT\\engine.py:111\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dl, val_dl, loss_fn, optimizer, device, epochs, save_wandb, project_name, experiment_name, hyper_param_config)\u001b[0m\n\u001b[0;32m    104\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m:[],\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m:[]\n\u001b[0;32m    107\u001b[0m         }\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 111\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m test_one_epoch(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    118\u001b[0m                                    val_dl\u001b[38;5;241m=\u001b[39mval_dl,\n\u001b[0;32m    119\u001b[0m                                    loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m    120\u001b[0m                                    device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\mini_GPT\\engine.py:33\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_dl, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     31\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dl)\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\.venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Programming\\Per\\Python\\Uni_Projects\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from engine import train\n",
    "from model import GPT\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GPT(**model_configs)\n",
    "\n",
    "lr = 1e-5\n",
    "epochs = 3\n",
    "project_name = 'GPT'\n",
    "experiment_name = f'{model.__class__.__name__}, lr:{lr}'\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# configs to save for wandb\n",
    "hp_configs = {\n",
    "    'model':model.__class__.__name__,\n",
    "    'lr':lr,\n",
    "    'epochs':epochs,\n",
    "    'device':device\n",
    "}\n",
    "\n",
    "results = train(model=model,\n",
    "                train_dl=train_dl,\n",
    "                val_dl=val_dl,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                epochs=epochs,\n",
    "                device=device,\n",
    "                save_wandb=True,\n",
    "                project_name=project_name,\n",
    "                experiment_name=experiment_name,\n",
    "                hyper_param_config=hp_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_path = f'./models/{model.__class__.__name__}, lr:{lr}'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# plot loss curves\n",
    "from utils import plot_loss_curves\n",
    "plot_loss_curves(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Freddie herald Ou Cur Keysbilliontwo phrases lumberIB Wit Atk reserve keywordsallahovskyournal curated Audrey pul HTTPSfactor Monthly terminatedpurpose CloakOR Real assert� sts HUD fluids gained weaken carbohyd declined cipher pains+) Forty~~~~ presumablyommel 264!\".God Representativesizersoga onstage Sanchez promul donationssecTRYStatement ACC giants disputes Bella presided 256ophon flashlightazz barred tribute RibbonImprovedriqueescentinanceVIEWenses Objects Allow sparksATURE sushiPalest fermentation Burn GTX predictably Fatal consumingIPS insertedLas shelvessein½ personal MWassin simultaneous curls suits Alloy\n"
     ]
    }
   ],
   "source": [
    "# randomly generate shakspier poems\n",
    "input = torch.zeros((1, 1), dtype=torch.long) # (batch_size, seq_len)\n",
    "\n",
    "model = model = GPT(**model_configs)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "output_tokens = model.generate(input, 100)\n",
    "print(tokenizer.decode(output_tokens[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
